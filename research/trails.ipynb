{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a2a4517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0ac7708",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspaces/Medical-Chatbot-Gen-AI/research'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "621d9c92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspaces/Medical-Chatbot-Gen-AI'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"/workspaces/Medical-Chatbot-Gen-AI\")\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac19cead",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7aa991f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdf_file(data):\n",
    "    loader= DirectoryLoader(data,glob=\"*.pdf\",loader_cls=PyPDFLoader)\n",
    "    documents=loader.load()\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0eeb7af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_data=load_pdf_file(data='Data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "475ac309",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_split(extracted_data):\n",
    "    text_splitter=RecursiveCharacterTextSplitter(chunk_size=100,chunk_overlap=20)\n",
    "    text_chunks=text_splitter.split_documents(extracted_data)\n",
    "    return text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14216ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunks=text_split(extracted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a7d5228",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22601/348322451.py:3: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "def download_huggingface_embeddings():\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    return embeddings\n",
    "\n",
    "# Correct function call\n",
    "embeddings = download_huggingface_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce128aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length 384\n"
     ]
    }
   ],
   "source": [
    "query_result=embeddings.embed_query(\"Hello World\")\n",
    "print(\"length\", len(query_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92cde6f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a08a4307",
   "metadata": {},
   "outputs": [],
   "source": [
    "PINECONE_API_KEY=os.environ.get('PINECONE_API_KEY')\n",
    "GEMINI_API_KEY=os.environ.get('GEMINI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4319491d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pinecone in /usr/local/python/3.12.1/lib/python3.12/site-packages (7.3.0)\n",
      "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pinecone) (2025.8.3)\n",
      "Requirement already satisfied: pinecone-plugin-assistant<2.0.0,>=1.6.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pinecone) (1.7.0)\n",
      "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pinecone) (0.0.7)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pinecone) (2.9.0.post0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pinecone) (4.14.1)\n",
      "Requirement already satisfied: urllib3>=1.26.5 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pinecone) (2.5.0)\n",
      "Requirement already satisfied: packaging<25.0,>=24.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pinecone-plugin-assistant<2.0.0,>=1.6.0->pinecone) (24.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.32.3 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pinecone-plugin-assistant<2.0.0,>=1.6.0->pinecone) (2.32.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from requests<3.0.0,>=2.32.3->pinecone-plugin-assistant<2.0.0,>=1.6.0->pinecone) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from requests<3.0.0,>=2.32.3->pinecone-plugin-assistant<2.0.0,>=1.6.0->pinecone) (3.10)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from python-dateutil>=2.5.3->pinecone) (1.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "002b0cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: pinecone\n",
      "Version: 7.3.0\n",
      "Summary: Pinecone client and SDK\n",
      "Home-page: https://www.pinecone.io\n",
      "Author: Pinecone Systems, Inc.\n",
      "Author-email: support@pinecone.io\n",
      "License: Apache-2.0\n",
      "Location: /usr/local/python/3.12.1/lib/python3.12/site-packages\n",
      "Requires: certifi, pinecone-plugin-assistant, pinecone-plugin-interface, python-dateutil, typing-extensions, urllib3\n",
      "Required-by: langchain-pinecone\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "523227d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/usr/local/python/3.12.1/lib/python312.zip', '/usr/local/python/3.12.1/lib/python3.12', '/usr/local/python/3.12.1/lib/python3.12/lib-dynload', '', '/home/codespace/.local/lib/python3.12/site-packages', '/usr/local/python/3.12.1/lib/python3.12/site-packages', '/tmp/tmpebuyjwq7']\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/pinecone/__init__.py 7.3.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.path)\n",
    "import pinecone\n",
    "print(pinecone.__file__, pinecone.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4c26743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/pinecone/__init__.py\n",
      "['Pinecone', 'PineconeApiAttributeError', 'PineconeApiException', 'PineconeApiKeyError', 'PineconeApiTypeError', 'PineconeApiValueError', 'PineconeAsyncio', 'PineconeConfig', 'PineconeConfigurationError', 'PineconeException', 'PineconeProtocolError']\n"
     ]
    }
   ],
   "source": [
    "import pinecone\n",
    "print(pinecone.__file__)   # should show .../site-packages/pinecone/__init__.py\n",
    "print([name for name in dir(pinecone) if \"Pinecone\" in name])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9cec6314",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone\n",
    "import os\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "index_name= \"medicalbot\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5b30d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PINECONE_API_KEY\"]=PINECONE_API_KEY\n",
    "os.environ[\"GEMINI_API_KEY\"]=GEMINI_API_KEY\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c305d8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "index_name=\"medicalbot\"\n",
    "docsearch= PineconeVectorStore.from_documents(\n",
    "    documents= text_chunks,\n",
    "    index_name=index_name,\n",
    "    embedding=embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f66cc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "index_name=\"medicalbot\"\n",
    "docsearch=PineconeVectorStore.from_existing_index(\n",
    "    index_name= index_name,\n",
    "    embedding= embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "80fe936e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_pinecone.vectorstores.PineconeVectorStore at 0x727779b90b30>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "62773a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever=docsearch.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":3})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4ae3a6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "llm=ChatGoogleGenerativeAI(model=\"gemini-2.5-pro\", google_api_key=os.environ[\"GEMINI_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6de152d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_prompt= (\n",
    "    \"you are an assistant for question answering tasks. \"\n",
    "    \"use the following pieces of retrieved context to answer \"\n",
    "    \"the questions. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use theree sentences maximum and keep the \"\n",
    "    \"answer concise\"\n",
    "    \"/n/n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
